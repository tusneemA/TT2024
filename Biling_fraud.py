# -*- coding: utf-8 -*-
"""Insurance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nhBPMytWEjq9dLHZm_8lf8L7DlLQgaq4

HealthCare Fraud Detection
"""

from google.colab import drive
drive.mount('/content/drive')

pip install scikit-learn-intelex

import pandas as pd
import numpy as np
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
import matplotlib.pyplot as plt
import missingno as msno
plt.style.use('ggplot')
from cp_clean_helper import benef_new_feats, benef_label_encode, show_values,\
in_out_na_replace, in_new_feats, out_new_feats, inout_label_encode, code_count, chr_cond_cnt
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', 100)
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearnex import patch_sklearn
patch_sklearn()
from sklearn import preprocessing
from sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score,GridSearchCV
from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import confusion_matrix, plot_roc_curve,roc_auc_score,f1_score
from sklearn.metrics import classification_report as cl_rep_skl
from imblearn.pipeline import Pipeline,make_pipeline
from imblearn.over_sampling import SMOTE,BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from yellowbrick.classifier import confusion_matrix, classification_report, ROCAUC
from yellowbrick.model_selection import CVScores
from cp_clean_helper import show_values
from LGR_helper import std_num_cols, rb_scale_cols, model_results, get_confusion_matrix
plt.style.use('ggplot')
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', 100)

"""### # Loading training and testing dataset:-"""

# Train datasets

train_y = pd.read_csv("/content/drive/MyDrive/Insurance/Train-1542865627584.csv")
train_beneficiary = pd.read_csv("/content/drive/MyDrive/Insurance/Train_Beneficiarydata-1542865627584.csv")
train_inpatient = pd.read_csv("/content/drive/MyDrive/Insurance/Train_Inpatientdata-1542865627584.csv")
train_outpatient = pd.read_csv("/content/drive/MyDrive/Insurance/Train_Outpatientdata-1542865627584.csv")

# Test datasets

test_provs = pd.read_csv("/content/drive/MyDrive/Insurance/Test-1542969243754.csv")
test_beneficiary = pd.read_csv("/content/drive/MyDrive/Insurance/Test_Beneficiarydata-1542969243754.csv")
test_inpatient = pd.read_csv("/content/drive/MyDrive/Insurance/Test_Inpatientdata-1542969243754.csv")
test_outpatient = pd.read_csv("/content/drive/MyDrive/Insurance/Train_Outpatientdata-1542865627584.csv")

train_inpatient.info()

train_beneficiary.info()

train_outpatient.info()

benef_label_encode(train_beneficiary)
benef_label_encode(test_beneficiary)

"""## Creating new features for beneficiary datasets:"""

benef_new_feats(train_beneficiary)
benef_new_feats(test_beneficiary)

train_beneficiary.info()

"""- Dropping DOD and IP/OP reimbursement and deductible columns"""

train_beneficiary.drop(['DOD','DOB','IPAnnualReimbursementAmt','OPAnnualReimbursementAmt',\
                      'IPAnnualDeductibleAmt','OPAnnualDeductibleAmt'], axis=1, inplace=True)\

test_beneficiary.drop(['DOD','DOB','IPAnnualReimbursementAmt','OPAnnualReimbursementAmt',\
                      'IPAnnualDeductibleAmt','OPAnnualDeductibleAmt'], axis=1, inplace=True)

"""## # Investigating Train and Test Inpatient info files:"""

train_inpatient.drop(['ClmProcedureCode_4','ClmProcedureCode_5','ClmProcedureCode_6'], axis=1, inplace=True)
test_inpatient.drop(['ClmProcedureCode_4','ClmProcedureCode_5','ClmProcedureCode_6'], axis=1, inplace=True)

code_count(train_inpatient)
code_count(test_inpatient)

train_inpatient.head(50)

"""- Imputing null values"""

in_out_na_replace(train_inpatient)
in_out_na_replace(test_inpatient)

"""## # Investigating train and test Outpatient info files:

- Removing columns with high null values:
"""

train_outpatient.drop(['ClmProcedureCode_4','ClmProcedureCode_5','ClmProcedureCode_6'], axis=1, inplace=True)
test_outpatient.drop(['ClmProcedureCode_4','ClmProcedureCode_5','ClmProcedureCode_6'], axis=1, inplace=True)

"""- Adding two features that counts codes per provider"""

code_count(train_outpatient)
code_count(test_outpatient)



"""- Imputing null values"""

in_out_na_replace(train_outpatient)
in_out_na_replace(test_outpatient)

"""- Looking at outliers

## Inpatient/Outpatient files EDA

- Distinct Beneficiaries

### Duplicate claims:

## Creating new features for inpatient/outpatient datasets:
"""

in_new_feats(train_inpatient)
in_new_feats(test_inpatient)
out_new_feats(train_outpatient)
out_new_feats(test_outpatient)

train_inpatient.head(5)

"""- Dropping ClaimStartDt, ClaimEndDt, AdmissionDt and DischargeDt columns"""

train_inpatient.drop(['ClaimStartDt','ClaimEndDt','AdmissionDt','DischargeDt'], axis=1, inplace=True)\

test_inpatient.drop(['ClaimStartDt','ClaimEndDt','AdmissionDt','DischargeDt'], axis=1, inplace=True)

train_outpatient.drop(['ClaimStartDt','ClaimEndDt'], axis=1, inplace=True)\

test_outpatient.drop(['ClaimStartDt','ClaimEndDt'], axis=1, inplace=True)

"""- Concatenating inpatient and outpatient datasets (train and test):"""

train_in_out_df = pd.concat([train_inpatient, train_outpatient],axis=0,ignore_index=True)
test_in_out_df = pd.concat([test_inpatient, test_outpatient],axis=0,ignore_index=True)

train_in_out_df.info()

# Train data
train_in_out_df['Hospital_Stay'] = train_in_out_df['Hospital_Stay'].fillna(-1)
train_in_out_df['Hospital_Stay'] = train_in_out_df['Hospital_Stay'].astype(int)

# Test data
test_in_out_df['Hospital_Stay'] = test_in_out_df['Hospital_Stay'].fillna(-1)
test_in_out_df['Hospital_Stay'] = test_in_out_df['Hospital_Stay'].astype(int)

"""- Label encoding code/ID columns:"""

inout_label_encode(train_in_out_df)
inout_label_encode(test_in_out_df)

"""- Merging beneficiary data with in_out train and test data"""

train_features = train_beneficiary.merge(train_in_out_df, on='BeneID')
clean_test = test_beneficiary.merge(test_in_out_df, on='BeneID')

train_features.info()

"""- Adding chronic count (per claim) feature to the complete data"""

chr_cond_cnt(train_features)
chr_cond_cnt(clean_test)

train_features.head(5)

clean_train = train_features.merge(train_y, on='Provider')

clean_train.info()

traindf = clean_train.drop(['BeneID','ClaimID','Provider'], axis=1)
testdf = clean_test.drop(['BeneID','ClaimID','Provider'], axis=1)

label_encoder = preprocessing.LabelEncoder()
traindf['PotentialFraud'] = label_encoder.fit_transform(traindf['PotentialFraud'])

## Separating x and y variables for test train split

LGR_x = traindf.drop(['PotentialFraud'], axis=1)
LGR_y = traindf['PotentialFraud']

# Train test 70:30 split

trainX, testX, trainY, testY = train_test_split(LGR_x, LGR_y, random_state=42,\
                                                shuffle=True, stratify=LGR_y, test_size=0.3)

trainX = trainX.drop(['NoOfMonths_PartACov', 'NoOfMonths_PartBCov','ChronicCond_Alzheimer',\
                            'ChronicCond_Heartfailure', 'ChronicCond_KidneyDisease',\
                            'ChronicCond_Cancer', 'ChronicCond_ObstrPulmonary',\
                            'ChronicCond_Depression', 'ChronicCond_Diabetes',\
                            'ChronicCond_IschemicHeart', 'ChronicCond_Osteoporasis',\
                            'ChronicCond_rheumatoidarthritis', 'ChronicCond_stroke'], axis=1)

testX = testX.drop(['NoOfMonths_PartACov', 'NoOfMonths_PartBCov','ChronicCond_Alzheimer',\
                          'ChronicCond_Heartfailure', 'ChronicCond_KidneyDisease',\
                          'ChronicCond_Cancer', 'ChronicCond_ObstrPulmonary',\
                          'ChronicCond_Depression', 'ChronicCond_Diabetes',\
                          'ChronicCond_IschemicHeart', 'ChronicCond_Osteoporasis',\
                          'ChronicCond_rheumatoidarthritis', 'ChronicCond_stroke'], axis=1)

base_logit = LogisticRegression(solver='liblinear')
 model_results(trainX, trainY, testX, testY, base_logit, show=True)

 base_logit = LogisticRegression(solver='liblinear', class_weight='balanced')
 model_results(trainX, trainY, testX, testY, base_logit, show=True)